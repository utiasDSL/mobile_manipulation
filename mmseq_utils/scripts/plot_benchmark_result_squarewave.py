import argparse
import os

import pandas
import numpy as np

from mmseq_utils.logging import DataLogger, DataPlotter, multipage
from mmseq_utils import math
import matplotlib.pyplot as plt
from pandas import DataFrame, concat
STATS = [("err_ee", "integral"), ("err_base_normalized", "integral"),
         ("err_ee_1", "integral"), ("err_base_normalized_1", "integral"),
         ("err_ee_2", "integral"), ("err_base_normalized_2", "integral"),
         ("cmd_accs_saturation", "mean"),
         ("cmd_jerks_base_linear", "max"),
         ("run_time", "mean"), ("constraints_violation", "mean")]

EE_HEIGHT = 0.708
REACHABLE_RADIUS = 1.8682347282929948

def get_optimal_base_err(r_bw_wd, r_ew_wd):
    r_bw_w_opt = (r_bw_wd - r_ew_wd[:2]) / np.linalg.norm(r_bw_wd - r_ew_wd[:2]) * REACHABLE_RADIUS + r_ew_wd[:2]

    return np.linalg.norm(r_bw_wd - r_bw_w_opt)

def construct_logger(path_to_folder):
    """ Path to data folder

    :param path_to_folder:
    :return:
    """
    items = os.listdir(path_to_folder)
    folder_num = 0
    file_num = 0
    for f in items:
        d = os.path.join(path_to_folder, f)
        if os.path.isdir(d):
            folder_num += 1
        else:
            file_num += 1

    if folder_num == 2:
        # if generated by running controller and simulator as two nodes
        # there will be two folders, one by controller and one by simulator
        return SquareWaveDataPlotter.from_ROSSIM_results(path_to_folder)
    elif folder_num == 1 and file_num ==0:
        # if generated by running controller in loop with simulator,
        # there is only one folder
        return SquareWaveDataPlotter.from_PYSIM_results(os.path.join(path_to_folder, items[0]))
    elif folder_num == 1 and file_num == 1:
        # if generated by running experiments on the real robot,
        # there is only one folder and one ros bag
        return SquareWaveDataPlotter.from_ROSEXP_results(path_to_folder)

    return print("Faile to construt logger at " + path_to_folder)

def statistics(plotters):

    stats_dict = {}
    for id, p in enumerate(plotters):
        stats = p.summary(STATS)
        stats_dict[p.data["name"]] = stats

    df = DataFrame.from_dict(stats_dict, orient='index', columns=[p[0] +"_"+ p[1]for p in STATS])

    return df

class SquareWaveDataPlotter(DataPlotter):
    def __init__(self, data, config=None):
        super().__init__(data, config)
        self._get_statistics_squarewave()

    def _get_statistics_squarewave(self):
        N = np.argwhere(np.linalg.norm(self.data["r_bw_w_ds"] - self.data["r_bw_w_ds"][-10], axis=1) < 1e-3).flatten()[0]
        self.data["err_ee_1"] = self.data["err_ee"][:N]
        self.data["err_ee_2"] = self.data["err_ee"][N:]
        # assume that controller achieves optimal error 10 steps before the second waypoint
        eb_opt = get_optimal_base_err(self.data["r_bw_w_ds"][-10], self.data["r_ew_w_ds"][-10])

        self.data["err_base_normalized_1"] = (self.data["err_base"][:N] - eb_opt) / (self.data["err_base"][0] - eb_opt)
        self.data["err_base_normalized_2"] = self.data["err_base"][N:] / self.data["err_base"][N]

        data_names = ["err_ee_1", "err_ee_2", "err_base_normalized_1", "err_base_normalized_2"]
        t1 = self.data["ts"][:N]
        t2 = self.data["ts"][N:]
        for name in data_names:
            stats = math.statistics(self.data[name])
            t = t1 if name.split("_")[-1] == "1" else t2
            self.data["statistics"][name] = {"rms": math.rms_continuous(t, np.abs(self.data[name])),
                                                   "integral": math.integrate_zoh(t, np.abs(self.data[name])),
                                                   "mean": stats[0], "max": stats[1],
                                                   "min": stats[2]}

    def plot_task_performance(self, axes=None, index=0, legend=None):
        if axes is None:
            f, axes = plt.subplots(4, 1, sharex=True)
        else:
            if len(axes) != 4:
                raise ValueError("Given axes number ({}) does not match task number ({}).".format(len(axes), 4))

        if legend is None:
            legend = self.data["name"]

        prop_cycle = plt.rcParams["axes.prop_cycle"]
        colors = prop_cycle.by_key()["color"]

        t_sim = self.data["ts"]
        nq = int(self.data["nq"])
        xs_sat, us_sat = self.model_interface.robot.checkBounds(self.data["xs"], self.data["cmd_accs"], -1e-3)
        axes[0].plot(t_sim, self.data["constraints_violation"]*100, label=legend + " mean = {:.3f}".format(self.data["statistics"]["constraints_violation"]["mean"]*100), color=colors[index])
        axes[0].set_ylabel("constraints violation (%)")

        axes[1].plot(t_sim, self.data["err_ee"],
                     label=legend + " acc = {:.3f}".format(self.data["statistics"]["err_ee"]["integral"]), color=colors[index])
        axes[1].set_ylabel("EE Err (m)")

        N = np.argwhere(np.linalg.norm(self.data["r_bw_w_ds"] - self.data["r_bw_w_ds"][-10], axis=1) < 1e-3).flatten()[0]
        axes[2].plot(t_sim[:N], self.data["err_base_normalized_1"],
                     label=legend + " part 1 acc = {:.3f}".format(self.data["statistics"]["err_base_normalized_1"]["integral"]), color=colors[index])
        axes[2].plot(t_sim[N:], self.data["err_base_normalized_2"], linestyle="--",
                     label=legend + " part 2 acc = {:.3f}".format(self.data["statistics"]["err_base_normalized_2"]["integral"]),
                     color=colors[index])
        axes[2].set_ylabel("Base Err (m)")

        axes[3].plot(t_sim, self.data["arm_manipulability"], label=legend)
        axes[3].set_ylabel("Arm Manipulability")
        axes[3].set_xlabel("Time (s)")

        for a in axes:
            a.legend()

        return axes


class BenchmarkDataPlotter():

    def __init__(self, folder_path):
        """

        :param folder_path: path to benchmark data folder
        :return:
        """
        # Generate statistics in each test result folder
        # NOTE: Decided not to generate plots here. It takes too much memory space. Generate plots using bash scripts instead.
        if "stats_all.csv" in os.listdir(folder_path):
            df = pandas.read_csv(os.path.join(folder_path, "stats_all.csv"), index_col=[0,1])
        else:
            dfs = []
            keys = []
            for folder_name in os.listdir(folder_path):
                test_folder_path = os.path.join(folder_path, folder_name)
                if os.path.isdir(test_folder_path):
                    plotters = []
                    for filename in os.listdir(test_folder_path):
                        d = os.path.join(test_folder_path, filename)
                        if os.path.isdir(d):
                            plotter = construct_logger(d)
                            plotters.append(plotter)

                    keys.append(folder_name)
                    dfs.append(statistics(plotters))

            # concatenate data frames
            df = concat(dfs, keys=keys)
            df.to_csv(os.path.join(folder_path, "stats_all.csv"))

        self.folder_path = folder_path
        self.df = df
        self.methods = self.df.index.get_level_values(1).unique().to_list()

    def summarize(self, methods):
        dfs = []
        for name in methods:
            if name in self.methods:
                dfs.append(self.df.xs(name, level=1))

        df_summary = concat([df.mean() for df in dfs], keys=methods, axis=1)
        df_summary.to_csv(os.path.join(self.folder_path, "stats_summary_{}.csv".format('_'.join(methods))))

        return dfs

    def plot(self, dfs, names):
        diff_ee = (dfs[0]["err_ee_integral"] - dfs[1]["err_ee_integral"]) / dfs[1]["err_ee_integral"] * 100
        diff_base = (dfs[0]["err_base_integral"] - dfs[1]["err_base_integral"]) / dfs[1]["err_base_integral"] * 100
        dJ = [diff_ee.to_numpy() , diff_base.to_numpy()]

        f, axes = plt.subplots(1, 1)
        plt.scatter(dJ[0], dJ[1])
        axes.set_xlabel("ΔJ_1 (%)")
        axes.set_ylabel("ΔJ_2 (%)")
        axes.set_title("{} vs {} on Lexicographic Optimality".format(*names))

    def plot_mean_error(self, dfs, names):
        """

        :param dfs: list of two dataframes
        :param names:
        :return:
        """
        num_dfs = len(dfs)
        # End Effector
        f1, axes1 = plt.subplots(2, 1, sharex=True)
        for i in range(num_dfs):
            axes1[0].hist(dfs[i]["err_ee_1_integral"], bins=100, label=names[i], alpha=0.5)
            axes1[1].hist(dfs[i]["err_ee_2_integral"], bins=100, label=names[i], alpha=0.5)

        axes1[0].set_title("t = 0 ~ 8s")
        axes1[1].set_title("t = 8 ~ 16s")
        axes1[1].set_xlabel("mean end effector tracking error (m)")

        # Base
        f2, axes2 = plt.subplots(2, 1, sharex=True)
        for i in range(num_dfs):
            axes2[0].hist(dfs[i]["err_base_normalized_1_integral"], bins=100, label=names[i], alpha=0.5)
            axes2[1].hist(dfs[i]["err_base_normalized_2_integral"], bins=100, label=names[i], alpha=0.5)

        axes2[0].set_title("t = 0 ~ 8s")
        axes2[1].set_title("t = 8 ~ 16s")
        axes2[1].set_xlabel("mean base tracking error (normalized)")
        for k in range(2):
            axes1[k].legend()
            axes2[k].legend()

    def box_plot(self, data_name, dfs, df_names, title=None):
        if title is None:
            title = data_name
        data = [df[data_name] for df in dfs]

        plt.figure()
        plt.boxplot(data, labels=df_names)
        plt.title(title)


def benchmark(folder_path):
    data_plotter = BenchmarkDataPlotter(folder_path)
    methods = ["HTIDKC", "HTMPC"]

    dfs = data_plotter.summarize(methods)
    data_plotter.plot_mean_error(dfs, methods)
    data_plotter.box_plot("cmd_jerks_base_linear_max", dfs, methods, "Maximum Commanded Base Jerk (Linear) (m/s^3)")
    data_plotter.box_plot("run_time_mean", dfs, methods, "Run Time (s)")

    plt.show()

def tracking(folder_path):
    plotters = []
    for filename in os.listdir(folder_path):
        d = os.path.join(folder_path, filename)
        if os.path.isdir(d):
            plotter = construct_logger(d)
            plotters.append(plotter)

    axes = None
    for pid, plotter in enumerate(plotters):
        axes = plotter.plot_task_performance(axes=axes, index=pid)

    plt.show()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-f', "--folder", required=True, help="Path to data folder.")
    parser.add_argument("--compare", action="store_true",
                        help="plot comparisons")
    parser.add_argument("--tracking", action="store_true",
                        help="plot tracking comparisons")
    args = parser.parse_args()
    if args.tracking:
        tracking(args.folder)
    else:
        benchmark(args.folder)


